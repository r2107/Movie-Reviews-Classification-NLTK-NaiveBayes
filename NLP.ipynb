{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is the thing which i always wanted to do!', 'whooo wait what?', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "sample = \"This is the thing which i always wanted to do! Whooo Wait what? nothing\"\n",
    "# sent to seperate as a sentences\n",
    "print(sent_tokenize(sample.lower()))\n",
    "# seperate as word wise\n",
    "words = word_tokenize(sample.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punct = list(string.punctuation)\n",
    "print(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thing', 'always', 'wanted', 'whooo', 'wait', 'nothing']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english') # contain all in small letters means not contain This but contain this\n",
    "stop = stop + punct\n",
    "clean_words = [w for w in words if w not in stop]\n",
    "clean_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming (player, playing, played, play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['player', 'play', 'play', 'play', 'rahul', 'happii', 'happier']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "lis = ['Player', 'Play', 'Playing', 'played', 'rahul', 'happii', 'happier'] # player and play bith have diff context\n",
    "stemmed = [ps.stem(w) for w in lis]\n",
    "print(stemmed) # this can produce even those words which are even not part of the english dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech (whether the word used is a nou,adjctive,verb,pronoun etc,,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import state_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = state_union.raw(\"2006-GWBush.txt\")\n",
    "pos = pos_tag(word_tokenize(text)) # needed to convert in the list\n",
    "# print(pos) # to see each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nltk.help.upenn_tagset() # to see what tag meaning is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('raj', 'NN'), ('went', 'VBD'), ('for', 'IN'), ('a', 'DT'), ('walk', 'NN')]\n",
      "[('This', 'DT'), ('painting', 'NN'), ('is', 'VBZ'), ('beautiful', 'JJ'), ('.', '.')]\n",
      "[('I', 'PRP'), ('have', 'VBP'), ('been', 'VBN'), ('painting', 'VBG'), ('since', 'IN'), ('morning', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag([\"raj\", \"went\", \"for\", \"a\", \"walk\"]))\n",
    "print(pos_tag([\"This\", \"painting\", \"is\", \"beautiful\", \".\"]))\n",
    "str1 = \"I have been painting since morning.\"\n",
    "print(pos_tag(word_tokenize(str1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmitizer(more powerfull way than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "good\n",
      "excellent\n",
      "painting\n",
      "paint\n"
     ]
    }
   ],
   "source": [
    "print(lem.lemmatize(\"better\", pos = 'a'))\n",
    "print(lem.lemmatize(\"good\", pos = 'a'))\n",
    "print(lem.lemmatize(\"excellent\", pos = 'n'))\n",
    "print(lem.lemmatize(\"painting\", pos = 'n')) # This painting is awesome\n",
    "print(lem.lemmatize(\"painting\", pos = 'v')) # i am painting this wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "# lemmatizer need part of speech in a diff format like for a noun we need to pass 'n'\n",
    "def simpler_pos(word):\n",
    "    if word.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif word.startswith('V'):\n",
    "        return wordnet.VERB # this constant defines in word net  from which we used lemmatizer\n",
    "    elif word.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN #not actually just use it as for simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Reviews Dataset Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n",
      "2000\n",
      "1000\n",
      "['best', 'remembered', 'for', 'his', 'understated', ...]\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.categories()) # all categories  in the dataset\n",
    "print(len(movie_reviews.fileids()))\n",
    "print(len(movie_reviews.fileids('neg'))) # lenth of negative reviews\n",
    "print(movie_reviews.words(movie_reviews.fileids()[10])) # accessing the words written in a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...], 'neg'),\n",
       " (['the', 'happy', 'bastard', \"'\", 's', 'quick', 'movie', ...], 'neg'),\n",
       " (['it', 'is', 'movies', 'like', 'these', 'that', 'make', ...], 'neg'),\n",
       " (['\"', 'quest', 'for', 'camelot', '\"', 'is', 'warner', ...], 'neg'),\n",
       " (['synopsis', ':', 'a', 'mentally', 'unstable', 'man', ...], 'neg')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [] # store all 2000 reviews with category in a tuple\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append((movie_reviews.words(fileid), category))\n",
    "documents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['house', 'on', 'haunted', 'hill', '(', '1999', ')', ...], 'neg'),\n",
       " (['here', \"'\", 's', 'a', 'concept', '--', 'jean', '-', ...], 'neg'),\n",
       " (['dr', '.', 'alan', 'grant', '(', 'sam', 'neill', ',', ...], 'neg'),\n",
       " (['old', 'soldiers', 'never', 'die', ',', 'they', ...], 'neg'),\n",
       " (['this', 'movie', 'is', 'written', 'by', 'the', 'man', ...], 'neg')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(documents) # to shuffle t randomly for easy train and test split\n",
    "documents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(words):\n",
    "    output = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stop:\n",
    "            pos = pos_tag([w])#pass word as array elem nt as strg becz then it consider each char of string as seprate elem\n",
    "            clean_word = lem.lemmatize(w, pos = simpler_pos(pos[0][1])) # as pos_tag will give an tuple 1st te answer 2nd its tag\n",
    "            output.append(clean_word.lower())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "documen = [(clean_review(document), category) for document, category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = documen[0:1500]\n",
    "test_doc = documen[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for doc in train_doc:\n",
    "    all_words += doc[0] # doc is a tuple with all words at its 0th entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "freq = nltk.FreqDist(all_words) # to store all frequencies\n",
    "common = freq.most_common(3000)\n",
    "features = [i[0] for i in common] #  it is an array of tuples each doc has there feat and category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_dict(words):\n",
    "    current_feat = {}\n",
    "    words_set = set(words)\n",
    "    for w in features:\n",
    "        current_feat[w] = w in words_set\n",
    "    return current_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [(get_features_dict(doc), category) for doc, category in train_doc]\n",
    "testing_data = [(get_features_dict(doc), category) for doc, category in test_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.746"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(training_data)\n",
    "nltk.classify.accuracy (classifier, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             magnificent = True              pos : neg    =     11.2 : 1.0\n",
      "                    anna = True              pos : neg    =     10.6 : 1.0\n",
      "              schumacher = True              neg : pos    =      9.5 : 1.0\n",
      "                    zeta = True              neg : pos    =      9.5 : 1.0\n",
      "            breathtaking = True              pos : neg    =      8.6 : 1.0\n",
      "             outstanding = True              pos : neg    =      8.1 : 1.0\n",
      "                religion = True              pos : neg    =      8.0 : 1.0\n",
      "               ludicrous = True              neg : pos    =      7.7 : 1.0\n",
      "                   anger = True              pos : neg    =      7.4 : 1.0\n",
      "             wonderfully = True              pos : neg    =      7.3 : 1.0\n",
      "                  seagal = True              neg : pos    =      7.2 : 1.0\n",
      "                  random = True              neg : pos    =      7.1 : 1.0\n",
      "                 balance = True              pos : neg    =      7.0 : 1.0\n",
      "                 idiotic = True              neg : pos    =      6.9 : 1.0\n",
      "                   jolie = True              neg : pos    =      6.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
