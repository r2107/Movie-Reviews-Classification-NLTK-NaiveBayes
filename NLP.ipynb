{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is the thing which i always wanted to do!', 'whooo wait what?', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "sample = \"This is the thing which i always wanted to do! Whooo Wait what? nothing\"\n",
    "# sent to seperate as a sentences\n",
    "print(sent_tokenize(sample.lower()))\n",
    "# seperate as word wise\n",
    "words = word_tokenize(sample.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punct = list(string.punctuation)\n",
    "print(punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thing', 'always', 'wanted', 'whooo', 'wait', 'nothing']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english') # contain all in small letters means not contain This but contain this\n",
    "stop = stop + punct\n",
    "clean_words = [w for w in words if w not in stop]\n",
    "clean_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming (player, playing, played, play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['player', 'play', 'play', 'play', 'rahul', 'happii', 'happier']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "lis = ['Player', 'Play', 'Playing', 'played', 'rahul', 'happii', 'happier'] # player and play bith have diff context\n",
    "stemmed = [ps.stem(w) for w in lis]\n",
    "print(stemmed) # this can produce even those words which are even not part of the english dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech (whether the word used is a nou,adjctive,verb,pronoun etc,,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import state_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = state_union.raw(\"2006-GWBush.txt\")\n",
    "pos = pos_tag(word_tokenize(text)) # needed to convert in the list\n",
    "# print(pos) # to see each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nltk.help.upenn_tagset() # to see what tag meaning is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('raj', 'NN'), ('went', 'VBD'), ('for', 'IN'), ('a', 'DT'), ('walk', 'NN')]\n",
      "[('This', 'DT'), ('painting', 'NN'), ('is', 'VBZ'), ('beautiful', 'JJ'), ('.', '.')]\n",
      "[('I', 'PRP'), ('have', 'VBP'), ('been', 'VBN'), ('painting', 'VBG'), ('since', 'IN'), ('morning', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag([\"raj\", \"went\", \"for\", \"a\", \"walk\"]))\n",
    "print(pos_tag([\"This\", \"painting\", \"is\", \"beautiful\", \".\"]))\n",
    "str1 = \"I have been painting since morning.\"\n",
    "print(pos_tag(word_tokenize(str1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmitizer(more powerfull way than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "good\n",
      "excellent\n",
      "painting\n",
      "paint\n"
     ]
    }
   ],
   "source": [
    "print(lem.lemmatize(\"better\", pos = 'a'))\n",
    "print(lem.lemmatize(\"good\", pos = 'a'))\n",
    "print(lem.lemmatize(\"excellent\", pos = 'n'))\n",
    "print(lem.lemmatize(\"painting\", pos = 'n')) # This painting is awesome\n",
    "print(lem.lemmatize(\"painting\", pos = 'v')) # i am painting this wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "# lemmatizer need part of speech in a diff format like for a noun we need to pass 'n'\n",
    "def simpler_pos(word):\n",
    "    if word.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif word.startswith('V'):\n",
    "        return wordnet.VERB # this constant defines in word net  from which we used lemmatizer\n",
    "    elif word.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN #not actually just use it as for simplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Reviews Dataset Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n",
      "2000\n",
      "1000\n",
      "['best', 'remembered', 'for', 'his', 'understated', ...]\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.categories()) # all categories  in the dataset\n",
    "print(len(movie_reviews.fileids()))\n",
    "print(len(movie_reviews.fileids('neg'))) # lenth of negative reviews\n",
    "print(movie_reviews.words(movie_reviews.fileids()[10])) # accessing the words written in a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...], 'neg'),\n",
       " (['the', 'happy', 'bastard', \"'\", 's', 'quick', 'movie', ...], 'neg'),\n",
       " (['it', 'is', 'movies', 'like', 'these', 'that', 'make', ...], 'neg'),\n",
       " (['\"', 'quest', 'for', 'camelot', '\"', 'is', 'warner', ...], 'neg'),\n",
       " (['synopsis', ':', 'a', 'mentally', 'unstable', 'man', ...], 'neg')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [] # store all 2000 reviews with category in a tuple\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append((movie_reviews.words(fileid), category))\n",
    "documents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['despite', 'its', 'exceedingly', 'well', '-', 'done', ...], 'neg'),\n",
       " (['well', 'if', 'you', 'are', 'up', 'for', 'stellar', ...], 'neg'),\n",
       " (['by', 'the', 'time', 'dennis', 'quaid', ',', 'the', ...], 'neg'),\n",
       " (['the', 'last', 'steve', 'martin', 'film', 'i', 'saw', ...], 'pos'),\n",
       " (['it', \"'\", 's', 'tough', 'to', 'really', 'say', ...], 'pos')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(documents) # to shuffle t randomly for easy train and test split\n",
    "documents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(words):\n",
    "    output = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stop:\n",
    "            pos = pos_tag([w])#pass word as array elem nt as strg becz then it consider each char of string as seprate elem\n",
    "            clean_word = lem.lemmatize(w, pos = simpler_pos(pos[0][1])) # as pos_tag will give an tuple 1st te answer 2nd its tag\n",
    "            output.append(clean_word.lower())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "documen = [(clean_review(document), category) for document, category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = documen[0:1500]\n",
    "test_doc = documen[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for doc in train_doc:\n",
    "    all_words += doc[0] # doc is a tuple with all words at its 0th entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "freq = nltk.FreqDist(all_words) # to store all frequencies\n",
    "common = freq.most_common(3000)\n",
    "features = [i[0] for i in common] #  it is an array of tuples each doc has there feat and category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_dict(words):\n",
    "    current_feat = {}\n",
    "    words_set = set(words)\n",
    "    for w in features:\n",
    "        current_feat[w] = w in words_set\n",
    "    return current_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [(get_features_dict(doc), category) for doc, category in train_doc]\n",
    "testing_data = [(get_features_dict(doc), category) for doc, category in test_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.818"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(training_data)\n",
    "nltk.classify.accuracy (classifier, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  seagal = True              neg : pos    =     10.9 : 1.0\n",
      "                 freddie = True              neg : pos    =     10.9 : 1.0\n",
      "             outstanding = True              pos : neg    =     10.7 : 1.0\n",
      "                  sloppy = True              neg : pos    =     10.5 : 1.0\n",
      "                  prinze = True              neg : pos    =     10.2 : 1.0\n",
      "              schumacher = True              neg : pos    =      9.5 : 1.0\n",
      "                   inept = True              neg : pos    =      9.2 : 1.0\n",
      "                   mulan = True              pos : neg    =      9.1 : 1.0\n",
      "             wonderfully = True              pos : neg    =      8.6 : 1.0\n",
      "                   ideal = True              pos : neg    =      7.8 : 1.0\n",
      "            respectively = True              pos : neg    =      7.4 : 1.0\n",
      "                   anger = True              pos : neg    =      6.2 : 1.0\n",
      "                 idiotic = True              neg : pos    =      6.0 : 1.0\n",
      "                     sat = True              neg : pos    =      5.8 : 1.0\n",
      "                   fiona = True              pos : neg    =      5.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SkLearn's Classifiers within NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC()\n",
    "classifier_sklearn = SklearnClassifier(svc)\n",
    "classifier_sklearn.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.786"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_sklearn, testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MrLather\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "classifier_sklearn1 = SklearnClassifier(rfc)\n",
    "classifier_sklearn1.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.694"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_sklearn1, testing_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
